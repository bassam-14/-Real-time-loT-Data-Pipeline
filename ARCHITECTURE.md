# Project Architecture & Setup Flow

## ğŸ“ File Structure & Purpose

```
final project/
â”‚
â”œâ”€â”€ ğŸ³ Docker Configuration
â”‚   â”œâ”€â”€ Dockerfile              â† Builds custom Airflow image with requirements.txt
â”‚   â”œâ”€â”€ docker-compose.yaml     â† Defines all services (Airflow, Kafka, MySQL, etc.)
â”‚   â”œâ”€â”€ .dockerignore          â† Excludes files from Docker build
â”‚   â””â”€â”€ requirements.txt        â† Python packages to install in Docker image
â”‚
â”œâ”€â”€ âš™ï¸ Configuration Files
â”‚   â”œâ”€â”€ .env                    â† Your secrets (create from sample.env)
â”‚   â”œâ”€â”€ sample.env             â† Template for environment variables
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ airflow.cfg        â† Airflow configuration
â”‚
â”œâ”€â”€ ğŸ“ Application Code
â”‚   â”œâ”€â”€ dags/                  â† Your Airflow DAG files go here
â”‚   â”‚   â””â”€â”€ Datagenerator.py   â† Sample sensor data generator
â”‚   â”œâ”€â”€ plugins/               â† Custom Airflow plugins
â”‚   â””â”€â”€ logs/                  â† Application logs (auto-generated)
â”‚
â”œâ”€â”€ ğŸ› ï¸ Setup & Build Scripts
â”‚   â”œâ”€â”€ setup.ps1              â† Windows: Full setup (builds + initializes + starts)
â”‚   â”œâ”€â”€ setup.sh               â† Linux/Mac: Full setup
â”‚   â”œâ”€â”€ build.ps1              â† Windows: Just build Docker image
â”‚   â”œâ”€â”€ build.sh               â† Linux/Mac: Just build Docker image
â”‚   â””â”€â”€ commands.ps1           â† PowerShell helper functions
â”‚
â”œâ”€â”€ ğŸ§ª Utilities
â”‚   â””â”€â”€ utils.py               â† Testing and helper script
â”‚
â””â”€â”€ ğŸ“š Documentation
    â”œâ”€â”€ README.md              â† Main project documentation
    â”œâ”€â”€ TEAM_SETUP.md          â† Team member onboarding guide
    â”œâ”€â”€ QUICK_REFERENCE.md     â† Command cheat sheet
    â”œâ”€â”€ REQUIREMENTS_EXPLAINED.md â† How requirements.txt works
    â”œâ”€â”€ SETUP_SUMMARY.md       â† What was configured
    â””â”€â”€ ARCHITECTURE.md        â† This file
```

## ğŸ”„ How Everything Connects

### 1ï¸âƒ£ Build Phase (Requirements.txt â†’ Docker Image)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  requirements.txt   â”‚  List of Python packages
â”‚  - kafka-python     â”‚  (pandas, pymysql, kafka, etc.)
â”‚  - pymysql          â”‚
â”‚  - confluent-kafka  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Used by
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Dockerfile       â”‚  Instructions to build custom image
â”‚  FROM airflow:3.1   â”‚  1. Start from official Airflow
â”‚  COPY requirements  â”‚  2. Copy requirements.txt
â”‚  RUN pip install   â”‚  3. Install all packages
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Executed by
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  docker-compose     â”‚  Build command creates image
â”‚      build          â”‚  docker-compose build
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Creates
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ iot-airflow:latest  â”‚  Custom Docker image with
â”‚  - Airflow 3.1      â”‚  all your packages installed
â”‚  - All packages     â”‚  Ready to use!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2ï¸âƒ£ Runtime Phase (Docker Image â†’ Services)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         docker-compose.yaml                         â”‚
â”‚  Defines all services and uses the custom image     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ docker-compose up -d
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  iot-airflow:latest â”‚  Custom image
    â”‚  (with packages)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ Used by all Airflow services
              â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
              â–¼      â–¼      â–¼      â–¼      â–¼
    â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”
    â”‚API  â”‚ â”‚Schedâ”‚ â”‚Workâ”‚ â”‚Trigâ”‚ â”‚DAG â”‚
    â”‚Serv â”‚ â”‚uler â”‚ â”‚er  â”‚ â”‚ger â”‚ â”‚Procâ”‚
    â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜
    All have access to kafka-python, pymysql, etc.
```

### 3ï¸âƒ£ Complete System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  IoT Data Pipeline System                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Host Machine (Windows/Linux/Mac)                        â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Docker Desktop                                  â”‚    â”‚
â”‚  â”‚                                                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ Network: default                       â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                                        â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚   Zookeeper  â”‚  â”‚    Kafka     â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚   :2181      â”‚â—„â”€â”¤   :9092      â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                            â”‚          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚   Kafka UI   â”‚  â”‚  Airflow     â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚   :8090      â”‚  â”‚  Services    â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  :8080       â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                    â”‚              â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â€¢ API Serverâ”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    MySQL     â”‚â—„â”€â”¤ â€¢ Scheduler â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    :3306     â”‚  â”‚ â€¢ Worker    â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ Triggerer â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚                    â”‚ â€¢ DAG Proc  â”‚  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â””â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    Redis     â”‚         â”‚          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚    :6379     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                           â”‚
â”‚  Volumes (persistent data):                              â”‚
â”‚  â€¢ mysql-db-volume  â†’ MySQL data                         â”‚
â”‚  â€¢ ./dags          â†’ DAG files                           â”‚
â”‚  â€¢ ./logs          â†’ Application logs                    â”‚
â”‚  â€¢ ./plugins       â†’ Custom plugins                      â”‚
â”‚  â€¢ ./config        â†’ Configuration                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Access Points                                            â”‚
â”‚  â€¢ http://localhost:8080  â†’ Airflow UI                   â”‚
â”‚  â€¢ http://localhost:8090  â†’ Kafka UI                     â”‚
â”‚  â€¢ localhost:9092         â†’ Kafka (from host)            â”‚
â”‚  â€¢ localhost:3306         â†’ MySQL (from host)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow

### Streaming Path (Real-time)

```
Sensor Data Generator (DAG)
         â†“
    [Produce to Kafka Topic: iot_sensor_data]
         â†“
    Kafka Broker (Port 9092)
         â†“
    [Kafka Consumer (in Airflow DAG)]
         â†“
    Process & Transform
         â†“
    Store in MySQL
```

### Batch Processing Path

```
Airflow Scheduler (checks DAGs every 30s)
         â†“
    Trigger DAG Run
         â†“
    Airflow Worker executes tasks
         â†“
    Task 1: Read from MySQL
    Task 2: Process data
    Task 3: Generate reports
    Task 4: Store results
```

## ğŸš€ Setup Flow

### Initial Setup (One Time)

```
1. Clone repository
   â†“
2. Copy sample.env â†’ .env (edit credentials)
   â†“
3. Run setup.ps1 or setup.sh
   â”œâ”€ Check Docker running
   â”œâ”€ Create directories
   â”œâ”€ Build Docker image (uses requirements.txt)
   â”œâ”€ Initialize Airflow database
   â””â”€ Start all services
   â†“
4. Create Kafka topics
   â†“
5. Access UIs and start developing
```

### Daily Development Workflow

```
1. git pull (get latest code)
   â†“
2. docker-compose up -d (start if stopped)
   â†“
3. Add/Edit DAGs in dags/ folder
   â†“
4. Airflow auto-detects new DAGs (30-60s)
   â†“
5. Test in Airflow UI
   â†“
6. git commit & push changes
```

### Adding New Python Packages

```
1. Edit requirements.txt
   (add: new-package>=1.0.0)
   â†“
2. docker-compose down
   â†“
3. docker-compose build --no-cache
   â†“
4. docker-compose up -d
   â†“
5. Verify: docker-compose exec airflow-apiserver pip list
```

## ğŸ¯ Key Concepts

### Why Docker?

- âœ… Consistent environment across all team members
- âœ… Easy to start/stop entire stack
- âœ… Isolated from your host system
- âœ… Easy to scale and deploy

### Why Custom Dockerfile?

- âœ… Pre-install all Python packages
- âœ… Fast container startup
- âœ… Version-controlled dependencies
- âœ… Reproducible builds

### Why Docker Compose?

- âœ… Define multiple services in one file
- âœ… Manage service dependencies
- âœ… Easy networking between containers
- âœ… Single command to manage everything

## ğŸ“Š Service Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Zookeeper   â”‚  (No dependencies)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ requires
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Kafka     â”‚  (Needs Zookeeper)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MySQL     â”‚  (No dependencies)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Redis     â”‚  (No dependencies)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ All three required by
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow    â”‚  (Needs MySQL, Redis, Kafka)
â”‚   Services   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Docker Compose ensures services start in correct order using `depends_on` and health checks!

## ğŸ’¡ Understanding the Magic

**Question**: How does Airflow inside Docker access Kafka?

**Answer**:

- Containers on same Docker network can communicate
- They use **service names** as hostnames
- Airflow connects to `kafka:29092` (not `localhost:9092`)
- From host machine, you use `localhost:9092`

**Question**: Where is my data stored?

**Answer**:

- **MySQL data**: Docker volume `mysql-db-volume` (persists after restart)
- **DAG files**: `./dags` folder (your machine â†’ mounted in container)
- **Logs**: `./logs` folder (your machine â†’ mounted in container)
- **Container data**: Deleted when you run `docker-compose down -v`

**Question**: How do changes to DAGs take effect?

**Answer**:

- Airflow watches `./dags` folder
- New/changed files detected automatically (30-60 seconds)
- No need to restart services
- Just save file â†’ refresh Airflow UI

---

**Now you understand the complete architecture! ğŸ“**

Next: Read `REQUIREMENTS_EXPLAINED.md` for deep dive into how requirements.txt works.
