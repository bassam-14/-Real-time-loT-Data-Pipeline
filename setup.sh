#!/bin/bash
# Bash Setup Script for IoT Data Pipeline (Linux/Mac)
# Run this script to set up the environment on Linux/Mac

echo "ğŸš€ Setting up IoT Data Pipeline Environment..."

# Check if Docker is running
echo ""
echo "ğŸ“‹ Checking Docker..."
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Error: Docker is not running. Please start Docker and try again."
    exit 1
fi
echo "âœ… Docker is running"

# Create .env file if it doesn't exist
if [ ! -f ".env" ]; then
    echo ""
    echo "ğŸ“‹ Creating .env file from sample..."
    cp sample.env .env
    echo "âœ… .env file created. Please edit it with your credentials!"
    echo "âš ï¸  Remember to change default passwords!"
else
    echo ""
    echo "âœ… .env file already exists"
fi

# Create required directories
echo ""
echo "ğŸ“‹ Creating required directories..."
for dir in dags logs plugins config; do
    if [ ! -d "$dir" ]; then
        mkdir -p "$dir"
        echo "  âœ… Created $dir/"
    else
        echo "  âœ… $dir/ already exists"
    fi
done

# Set AIRFLOW_UID
echo ""
echo "ğŸ“‹ Setting AIRFLOW_UID..."
echo "AIRFLOW_UID=$(id -u)" >> .env
echo "âœ… AIRFLOW_UID set to $(id -u)"

# Fix permissions
echo ""
echo "ğŸ“‹ Setting correct permissions..."
sudo chown -R $(id -u):$(id -g) dags logs plugins config
echo "âœ… Permissions set"

# Initialize Airflow
echo ""
echo "ğŸ“‹ Initializing Airflow (this may take a few minutes)..."
docker-compose up airflow-init
if [ $? -eq 0 ]; then
    echo "âœ… Airflow initialized successfully"
else
    echo "âŒ Error initializing Airflow"
    exit 1
fi

# Start all services
echo ""
echo "ğŸ“‹ Starting all services..."
docker-compose up -d
if [ $? -eq 0 ]; then
    echo "âœ… All services started successfully"
else
    echo "âŒ Error starting services"
    exit 1
fi

# Wait for services to be healthy
echo ""
echo "â³ Waiting for services to be healthy (30 seconds)..."
sleep 30

# Check service status
echo ""
echo "ğŸ“‹ Service Status:"
docker-compose ps

# Print access information
echo ""
echo "âœ… Setup Complete!"
echo ""
echo "ğŸŒ Access URLs:"
echo "  â€¢ Airflow UI:  http://localhost:8080"
echo "    Username: airflow | Password: airflow (or your custom credentials)"
echo "  â€¢ Kafka UI:    http://localhost:8090"
echo "  â€¢ MySQL:       localhost:3306"
echo ""
echo "ğŸ“š Next Steps:"
echo "  1. Open Airflow UI at http://localhost:8080"
echo "  2. Create Kafka topics: docker-compose exec kafka kafka-topics --create --bootstrap-server localhost:29092 --topic iot_sensor_data --partitions 3"
echo "  3. Add your DAG files to the dags/ folder"
echo ""
echo "ğŸ’¡ Tips:"
echo "  â€¢ View logs: docker-compose logs -f"
echo "  â€¢ Stop services: docker-compose stop"
echo "  â€¢ Remove everything: docker-compose down -v"
echo ""
echo "Happy Data Engineering! ğŸš€"
