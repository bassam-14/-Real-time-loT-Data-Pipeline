# Team Setup Guide - IoT Data Pipeline Project

## üë• For Team Members

This guide will help you get started with the IoT Data Pipeline project.

## üìã Prerequisites Checklist

Before you begin, make sure you have:

- [ ] Docker Desktop installed and running
  - Windows: https://docs.docker.com/desktop/install/windows-install/
  - Mac: https://docs.docker.com/desktop/install/mac-install/
  - Linux: https://docs.docker.com/engine/install/
- [ ] Git installed
  - Download: https://git-scm.com/downloads
- [ ] At least 4GB RAM allocated to Docker
  - Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí Memory (set to 4GB+)
- [ ] At least 10GB free disk space

- [ ] Text editor (VS Code recommended)
  - Download: https://code.visualstudio.com/

## üöÄ Step-by-Step Setup (Windows)

### Step 1: Clone the Repository

```powershell
# Navigate to your workspace
cd "D:\Mohamred data\data engineering DEPI\Technical"

# Clone the repository (if not already done)
git clone https://github.com/bassam-14/-Real-time-loT-Data-Pipeline.git
cd -Real-time-loT-Data-Pipeline

# Or if already cloned, just navigate to it
cd "final project"
```

### Step 2: Create Your Environment File

```powershell
# Copy the sample environment file
Copy-Item sample.env .env

# Open .env in notepad or your preferred editor
notepad .env
```

**Edit the .env file and change these values:**

```bash
MYSQL_USER=your_username                    # Your MySQL username
MYSQL_PASSWORD=YourSecurePassword123       # Your MySQL password (CHANGE THIS!)
MYSQL_ROOT_PASSWORD=YourRootPassword123    # MySQL root password (CHANGE THIS!)
_AIRFLOW_WWW_USER_USERNAME=admin           # Airflow admin username
_AIRFLOW_WWW_USER_PASSWORD=AdminPass123    # Airflow admin password (CHANGE THIS!)
```

‚ö†Ô∏è **Important**: Never commit the .env file to Git!

### Step 3: Run the Setup Script

```powershell
# Run the automated setup script
.\setup.ps1
```

This script will:

- ‚úÖ Check if Docker is running
- ‚úÖ Create necessary directories
- ‚úÖ Initialize Airflow database
- ‚úÖ Start all services

**Wait 2-3 minutes** for all services to start and become healthy.

### Step 4: Verify Installation

Open your browser and check:

- Airflow UI: http://localhost:8080 (login with your credentials)
- Kafka UI: http://localhost:8090 (no login needed)

### Step 5: Create Kafka Topic

```powershell
# Create the main sensor data topic
docker-compose exec kafka kafka-topics --create `
  --bootstrap-server localhost:29092 `
  --replication-factor 1 `
  --partitions 3 `
  --topic iot_sensor_data

# Verify topic was created
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:29092
```

### Step 6: Test the Setup

```powershell
# Install Python dependencies locally (optional, for testing)
pip install -r requirements.txt

# Run the utility script to test everything
python utils.py
```

Select option 7 to run all tests.

## üöÄ Step-by-Step Setup (Linux/Mac)

### Step 1: Clone the Repository

```bash
cd ~/workspace
git clone https://github.com/bassam-14/-Real-time-loT-Data-Pipeline.git
cd -Real-time-loT-Data-Pipeline
```

### Step 2: Create Environment File

```bash
cp sample.env .env
nano .env  # or use vim, code, etc.
```

### Step 3: Run Setup Script

```bash
chmod +x setup.sh
./setup.sh
```

### Step 4-6: Same as Windows above

## üìÅ Project Structure

```
final project/
‚îú‚îÄ‚îÄ dags/                  # Put your Airflow DAG files here
‚îÇ   ‚îî‚îÄ‚îÄ Datagenerator.py  # Sample sensor data generator
‚îú‚îÄ‚îÄ logs/                  # Airflow logs (auto-generated)
‚îú‚îÄ‚îÄ plugins/               # Custom Airflow plugins
‚îú‚îÄ‚îÄ config/                # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ airflow.cfg       # Airflow configuration
‚îú‚îÄ‚îÄ docker-compose.yaml    # Docker services definition
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îú‚îÄ‚îÄ .env                   # Your environment variables (DO NOT COMMIT!)
‚îú‚îÄ‚îÄ sample.env            # Sample environment template
‚îú‚îÄ‚îÄ README.md             # Main documentation
‚îú‚îÄ‚îÄ QUICK_REFERENCE.md    # Quick command reference
‚îú‚îÄ‚îÄ TEAM_SETUP.md         # This file
‚îú‚îÄ‚îÄ setup.ps1             # Windows setup script
‚îú‚îÄ‚îÄ setup.sh              # Linux/Mac setup script
‚îî‚îÄ‚îÄ utils.py              # Utility testing script
```

## üîÑ Daily Workflow

### Starting Work

```powershell
# 1. Pull latest changes from Git
git pull origin Elshewy

# 2. Start Docker services (if not running)
docker-compose up -d

# 3. Check services are healthy
docker-compose ps
```

### During Development

```powershell
# Create/edit DAG files in dags/ folder
# Airflow will automatically detect changes in 30-60 seconds

# View logs if needed
docker-compose logs -f airflow-scheduler

# Test Kafka connection
python utils.py  # Select option 1
```

### Ending Work

```powershell
# 1. Commit your changes
git add .
git commit -m "Your commit message"
git push origin Elshewy

# 2. Stop services (optional - keeps data)
docker-compose stop

# Or keep them running for next session
```

## üîß Common Tasks

### Working with DAGs

1. **Create a new DAG**

   - Add a new Python file in `dags/` folder
   - Airflow will auto-detect it
   - Refresh the Airflow UI

2. **Test a DAG**

   ```powershell
   docker-compose exec airflow-apiserver airflow dags test <dag_id> 2025-10-05
   ```

3. **Trigger a DAG manually**
   - Go to Airflow UI ‚Üí DAGs ‚Üí Click the play button
   - Or use CLI: `docker-compose exec airflow-apiserver airflow dags trigger <dag_id>`

### Working with Kafka

1. **Send test data to Kafka**

   ```powershell
   # Use the utility script
   python utils.py  # Select option 4
   ```

2. **Monitor Kafka messages**

   - Open Kafka UI: http://localhost:8090
   - Go to Topics ‚Üí iot_sensor_data ‚Üí Messages

3. **Consume messages from terminal**
   ```powershell
   docker-compose exec kafka kafka-console-consumer `
     --bootstrap-server localhost:29092 `
     --topic iot_sensor_data `
     --from-beginning
   ```

### Working with MySQL

1. **Connect to MySQL**

   ```powershell
   docker-compose exec mysql mysql -u airflow -p
   ```

2. **Create tables**

   ```powershell
   # Use the utility script
   python utils.py  # Select option 3
   ```

3. **Query data**
   ```sql
   USE airflow_db;
   SHOW TABLES;
   SELECT * FROM sensor_readings LIMIT 10;
   ```

## ‚ùì Troubleshooting

### Problem: Services won't start

**Solution:**

```powershell
# Stop everything
docker-compose down -v

# Start fresh
docker-compose up airflow-init
docker-compose up -d
```

### Problem: "Port already in use"

**Solution:**

```powershell
# Check what's using the port
netstat -ano | findstr :8080  # Windows
lsof -i :8080                 # Mac/Linux

# Kill the process or change port in docker-compose.yaml
```

### Problem: Airflow UI shows connection errors

**Solution:**

```powershell
# Check MySQL is healthy
docker-compose logs mysql

# Wait for this message: "mysqld: ready for connections"
# Then restart Airflow services
docker-compose restart airflow-apiserver airflow-scheduler
```

### Problem: Can't see my DAG in Airflow UI

**Solution:**

1. Check the DAG file has no syntax errors
2. Check logs: `docker-compose logs airflow-scheduler`
3. Wait 60 seconds for auto-refresh
4. Make sure DAG file is in `dags/` folder
5. Check DAG isn't paused (toggle in UI)

### Problem: Kafka connection refused

**Solution:**

```powershell
# Check Kafka is healthy
docker-compose ps kafka

# Check Kafka logs
docker-compose logs kafka

# Restart Kafka
docker-compose restart zookeeper kafka
```

## üìû Getting Help

1. **Check the documentation**

   - README.md - Full project documentation
   - QUICK_REFERENCE.md - Command reference
   - This file - Setup guide

2. **Check logs**

   ```powershell
   docker-compose logs <service-name>
   ```

3. **Ask the team**
   - Create an issue on GitHub
   - Ask in team chat
   - Schedule a pair programming session

## üéØ Next Steps

After setup is complete:

1. ‚úÖ Familiarize yourself with the Airflow UI
2. ‚úÖ Explore the Kafka UI to understand topics
3. ‚úÖ Review the existing DAG file: `dags/Datagenerator.py`
4. ‚úÖ Read the project requirements document
5. ‚úÖ Start working on your assigned tasks

## üìö Learning Resources

- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Kafka Quickstart](https://kafka.apache.org/quickstart)
- [MySQL Tutorial](https://dev.mysql.com/doc/mysql-tutorial-excerpt/8.0/en/)
- [Docker Compose](https://docs.docker.com/compose/)

## üîê Security Reminders

- ‚ö†Ô∏è Never commit `.env` file to Git
- ‚ö†Ô∏è Change all default passwords
- ‚ö†Ô∏è Use strong passwords (12+ characters, mixed case, numbers, symbols)
- ‚ö†Ô∏è Don't share credentials in chat or email
- ‚ö†Ô∏è This setup is for development only - production needs additional security

## ‚úÖ Setup Checklist

After completing setup, verify:

- [ ] Docker services are running (`docker-compose ps`)
- [ ] Airflow UI is accessible (http://localhost:8080)
- [ ] Kafka UI is accessible (http://localhost:8090)
- [ ] Can login to Airflow with your credentials
- [ ] Kafka topic `iot_sensor_data` exists
- [ ] MySQL connection works
- [ ] Can see example DAG in Airflow UI
- [ ] All tests pass in `utils.py`

---

**Welcome to the team! Happy coding! üöÄ**

For questions or issues, contact the project maintainers or create a GitHub issue.
